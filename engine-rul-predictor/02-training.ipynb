{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fde8646-e7f5-477a-9fc3-93279f850152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "model_name = \"engine_rul_predictor\"\n",
    "username = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "mlflow.set_experiment(f'/Users/{username}/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a009ef92-f5dd-489f-afc7-1b63b488616e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = 'demos'\n",
    "database = \"engine_rul_predictor\"\n",
    "source_table = \"nasa_data_train_test\"\n",
    "target_table = \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d751adb0-e408-40ba-9325-bcb0d38bd8ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the default catalog and schema\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"USE {database}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e48abe-cfb2-411d-b860-307315b91b68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#Read the Training Data\n",
    "train_df = spark.table(f\"{database}.{source_table}\")\n",
    "\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14133462-369b-419a-8ef4-74998b59b556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Define window specifications\n",
    "rollingWindowSpec = Window.partitionBy(\"id\").orderBy(\"Cycle\").rowsBetween(-1, 0)\n",
    "lagWindowSpec = Window.partitionBy(\"id\").orderBy(\"Cycle\")\n",
    "\n",
    "# Additional feature engineering for all 21 sensors\n",
    "features_df = train_df\n",
    "# for i in range(1, 22):\n",
    "#     features_df = features_df.withColumn(\n",
    "#         f\"SensorMeasure{i}_rolling_avg\", \n",
    "#         F.avg(f\"SensorMeasure{i}\").over(rollingWindowSpec).cast(FloatType())\n",
    "#     ).withColumn(\n",
    "#         f\"SensorMeasure{i}_diff\", \n",
    "#         (F.col(f\"SensorMeasure{i}\") - F.lag(f\"SensorMeasure{i}\", 1).over(lagWindowSpec)).cast(FloatType())\n",
    "#     )\n",
    "\n",
    "# Save the features to a Delta table\n",
    "features_df.write.mode(\"overwrite\").saveAsTable(f\"{database}.{target_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72127cec-aa52-41cb-b679-58d2da2fd05f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import *\n",
    "mlflow.spark.autolog()\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame and drop unnecessary columns\n",
    "data = features_df.toPandas()\n",
    "\n",
    "# Split data into train and test sets\n",
    "train, test = train_test_split(data, test_size=0.30, random_state=206)\n",
    "colLabel = 'RemainingUsefulLife'\n",
    "\n",
    "# The predicted column is colLabel which is a scalar\n",
    "train_x = train.drop([colLabel], axis=1)\n",
    "test_x = test.drop([colLabel], axis=1)\n",
    "train_y = train[colLabel]\n",
    "test_y = test[colLabel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd8c629-e8c7-4280-a365-3a2492d650c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "# Train and log the model\n",
    "with mlflow.start_run(run_name=\"RUL_DecisionTreeRegressor\") as run:\n",
    "    # Define the model\n",
    "    model = DecisionTreeRegressor(random_state=206)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(train_x, train_y)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    predictions = model.predict(test_x)\n",
    "    \n",
    "    # Calculate error metrics\n",
    "    mae = mean_absolute_error(test_y, predictions)\n",
    "    mse = mean_squared_error(test_y, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(test_y, predictions)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\"mae\": mae, \"mse\": mse, \"rmse\": rmse, \"r2\": r2})\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(model, \"decision_tree_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48d7df8f-9452-45ca-a1cf-110be01a95b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the search space\n",
    "search_space = {\n",
    "    'max_depth': hp.choice('max_depth', range(1, 20)),\n",
    "    'max_leaf_nodes': hp.choice('max_leaf_nodes', range(4, 128))\n",
    "}\n",
    "\n",
    "def train_model(params):\n",
    "    mlflow.sklearn.autolog()\n",
    "    \n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Define the model\n",
    "        model = DecisionTreeRegressor(**params)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_x, train_y)\n",
    "        \n",
    "        # Predict on the test set\n",
    "        predictions = model.predict(test_x)\n",
    "        \n",
    "        # Calculate the loss (mean squared error)\n",
    "        loss = mean_squared_error(test_y, predictions)\n",
    "        \n",
    "        return {'status': STATUS_OK, 'loss': loss}\n",
    "\n",
    "# Start the hyperparameter tuning process\n",
    "with mlflow.start_run(run_name='sklearn_hyperopt') as run:\n",
    "    best_params = fmin(\n",
    "        fn=train_model,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=20,\n",
    "        trials=SparkTrials()\n",
    "    )\n",
    "\n",
    "run_id = run.info.run_uuid\n",
    "experiment_id = run.info.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43719d9c-c3a3-4bb2-95fe-d5980ae6462a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Read the experiment data\n",
    "experiment_Df = spark.read.format(\"mlflow-experiment\").load(experiment_id)\n",
    "\n",
    "# Find the best run based on MSE (lower is better)\n",
    "best_run = (\n",
    "  experiment_Df\n",
    "    .filter(experiment_Df.tags[\"mlflow.rootRunId\"] == run_id)\n",
    "    .orderBy(experiment_Df.metrics[\"mse\"].asc())  # Using 'mse' as the metric\n",
    "    .limit(1)\n",
    "    .first()['run_id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "235a0874-eed0-4a24-bdd8-6af9df07799e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20fde63d-a249-4bc0-8b83-2a3cbbbe2637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "model_uri = f\"runs:/{best_run}/model\"\n",
    "\n",
    "\n",
    "# print(model_uri)\n",
    "# Verify the artifact location\n",
    "local_path = mlflow.artifacts.download_artifacts(artifact_uri=model_uri)\n",
    "\n",
    "# Register the model\n",
    "model_details = mlflow.register_model(model_uri, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e3b5286-abbf-4e8a-8a5c-7ec370f4e11f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Set an alias for the model version\n",
    "client.set_registered_model_alias(\n",
    "    name=model_name,\n",
    "    alias=\"Production\",\n",
    "    version=model_details.version\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
